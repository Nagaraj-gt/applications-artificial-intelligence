{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "271b2f110f0f4f798b38326912ec1e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70f037f5b1134c4fbd080d7065390bd6",
              "IPY_MODEL_6a55edc7af334f809df26aa6f32b976e",
              "IPY_MODEL_21407d506e9a4c4fa6a48fc4990df5ce"
            ],
            "layout": "IPY_MODEL_d106aa9c86354f5dbe291868aebc12e3"
          }
        },
        "70f037f5b1134c4fbd080d7065390bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6237b47beb8f4c77bb358bc08c2af494",
            "placeholder": "​",
            "style": "IPY_MODEL_c26f058ddabc434da9fb61a448b1c203",
            "value": "100%"
          }
        },
        "6a55edc7af334f809df26aa6f32b976e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d99c5605bc340d1945181b26e54f70e",
            "max": 574673361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c56859a552c94569b53859bcfa307bb5",
            "value": 574673361
          }
        },
        "21407d506e9a4c4fa6a48fc4990df5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a6ef181e0a4ace9fdfd14175b39970",
            "placeholder": "​",
            "style": "IPY_MODEL_04872403cf9b465ebebe972e36543a68",
            "value": " 548M/548M [00:06&lt;00:00, 92.6MB/s]"
          }
        },
        "d106aa9c86354f5dbe291868aebc12e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6237b47beb8f4c77bb358bc08c2af494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26f058ddabc434da9fb61a448b1c203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d99c5605bc340d1945181b26e54f70e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56859a552c94569b53859bcfa307bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8a6ef181e0a4ace9fdfd14175b39970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04872403cf9b465ebebe972e36543a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagaraj-gt/applications-artificial-intelligence/blob/main/q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*# PART B (3) : Image Classification\n",
        "\n",
        "***Finetune the vgg19 model on this dataset, train a 15-class classification model and report \n",
        "per-class classification accuracy in terms of precision and recall. \n",
        " Submit q2.py. [10 marks]***\n",
        "\n",
        "\n",
        "\n",
        "**TEAM MEMBERS:**\n",
        "\n",
        " Nagaraj G T\t 12120095\n",
        "\n",
        " Yashaswi Singh\t 12120064\n",
        "\n",
        " Madhab Chakraborty\t 12120045\n",
        "\n",
        " Rama Gangadhar Durvasula\t 12120087\n",
        "\n",
        " Parmarth matta\t 12120077\n",
        "\n"
      ],
      "metadata": {
        "id": "JeWM8DJ0lcyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T3xlOgIAkeS5"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, models, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre=processing the input images to match images with what was presented during training period\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )])"
      ],
      "metadata": {
        "id": "k3QsPWTXjq75"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Train and Test Images. The Assignment data is split into train (1 to 40) and validation (rest) sets manually and uploaded in GIT\n",
        "\n",
        "!wget 'https://github.com/Nagaraj-gt/applications-artificial-intelligence/raw/main/dataset.zip'\n",
        "!unzip dataset.zip\n"
      ],
      "metadata": {
        "id": "_NCz9q7DkaMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries for training\n",
        "\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "UgnWYuDizohU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "g26dVGJZQmMB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/dataset'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcPZIRIExu1O",
        "outputId": "c7788dd8-5593-4512-e780-3443ef0fde5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to train pre-trained model with domain specific data\n",
        "import copy\n",
        "import time\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nhvkWVIHCQ-q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to visualize Model Predictions\n",
        "\n",
        "# Visualize few images\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "IN-uk_kEC0SD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG 19 Model\n"
      ],
      "metadata": {
        "id": "1lHAaqOpdWfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained VGG Model and reset final fully connected Layer for training\n",
        "\n",
        "vgg_model = models.vgg19(pretrained=True)\n",
        "num_ftrs = vgg_model.classifier[0].in_features\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "vgg_model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "model_ft = vgg_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "271b2f110f0f4f798b38326912ec1e52",
            "70f037f5b1134c4fbd080d7065390bd6",
            "6a55edc7af334f809df26aa6f32b976e",
            "21407d506e9a4c4fa6a48fc4990df5ce",
            "d106aa9c86354f5dbe291868aebc12e3",
            "6237b47beb8f4c77bb358bc08c2af494",
            "c26f058ddabc434da9fb61a448b1c203",
            "3d99c5605bc340d1945181b26e54f70e",
            "c56859a552c94569b53859bcfa307bb5",
            "c8a6ef181e0a4ace9fdfd14175b39970",
            "04872403cf9b465ebebe972e36543a68"
          ]
        },
        "outputId": "3db3c7bf-279f-4d3b-b618-19c3bcc042d1",
        "id": "nFSddNO1d0E5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "271b2f110f0f4f798b38326912ec1e52"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992ff1c5-0280-4369-9343-ab16e0c93523",
        "id": "5x4f8WsAd75q"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 3.8695 Acc: 0.0450\n",
            "val Loss: 2.8988 Acc: 0.0732\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 2.5794 Acc: 0.2000\n",
            "val Loss: 1.5939 Acc: 0.4146\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1.8446 Acc: 0.4317\n",
            "val Loss: 1.0890 Acc: 0.6585\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1.4546 Acc: 0.5783\n",
            "val Loss: 1.3159 Acc: 0.5756\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1.3446 Acc: 0.5850\n",
            "val Loss: 1.1978 Acc: 0.6439\n",
            "\n",
            "Training complete in 1m 52s\n",
            "Best val Acc: 0.658537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Per class classification accuracy report in terms of precision and recall\n",
        "\n",
        "import pandas as pd\n",
        "def print_accuracy_matrix(model):\n",
        "  nb_classes = 15\n",
        "\n",
        "  confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "  with torch.no_grad():\n",
        "      for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
        "          inputs = inputs.to(device)\n",
        "          classes = classes.to(device)\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          for t, p in zip(classes.view(-1), preds.view(-1)): \n",
        "                  confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "  precision = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
        "  recall = confusion_matrix.diag()/confusion_matrix.sum(0)\n",
        "  res_accuracy = pd.DataFrame(list(zip(class_names,precision.tolist(), recall.tolist())), columns=['Class', 'Precision', 'Recall'])\n",
        "\n",
        "  print(res_accuracy)"
      ],
      "metadata": {
        "id": "NMPwTLCrnEFw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy_matrix(model_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6EDe-1d5jja",
        "outputId": "6900fbc0-fd94-4ae0-94f3-9a9a8db50314"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Class  Precision    Recall\n",
            "0        accordion   0.933333  1.000000\n",
            "1             bass   0.142857  0.666667\n",
            "2           camera   0.900000  1.000000\n",
            "3        crocodile   0.900000  0.272727\n",
            "4   crocodile_head   0.000000       NaN\n",
            "5              cup   0.941176  0.941176\n",
            "6      dollar_bill   0.416667  1.000000\n",
            "7              emu   0.615385  0.727273\n",
            "8       gramophone   0.454545  0.625000\n",
            "9         hedgehog   0.857143  0.333333\n",
            "10        nautilus   0.866667  0.928571\n",
            "11           pizza   0.615385  0.888889\n",
            "12         pyramid   0.823529  0.933333\n",
            "13       sea_horse   0.235294  0.666667\n",
            "14   windsor_chair   1.000000  0.640000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION : The accuracy of VGG19 is about 72 - 75%. \n",
        "\n",
        "The accuracy is around 100% for precision and recall for certain classes like accordion and windsor_chair. However crocodile , bass its pretty bad !\n",
        "\n",
        "THis model needs to be fine tuned !\n"
      ],
      "metadata": {
        "id": "XFMTAgFpi3Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINETUNING OF VGG 19 MODEL FOR BETTER ACCURACIES"
      ],
      "metadata": {
        "id": "anJnT1FQBRZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 1 : Increase Epochs during training time"
      ],
      "metadata": {
        "id": "Lu_afk0WBsH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EXfNwewBdqp",
        "outputId": "b77f91cb-8df6-49ac-b0ec-de469e958a76"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.2846 Acc: 0.6083\n",
            "val Loss: 0.8012 Acc: 0.7366\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 1.4124 Acc: 0.5900\n",
            "val Loss: 0.8068 Acc: 0.7463\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.6960 Acc: 0.7767\n",
            "val Loss: 0.3944 Acc: 0.8927\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.5608 Acc: 0.8217\n",
            "val Loss: 0.3397 Acc: 0.8927\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.5078 Acc: 0.8400\n",
            "val Loss: 0.3002 Acc: 0.9268\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.5386 Acc: 0.8200\n",
            "val Loss: 0.2904 Acc: 0.9024\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.4351 Acc: 0.8467\n",
            "val Loss: 0.2538 Acc: 0.9073\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.4768 Acc: 0.8400\n",
            "val Loss: 0.2534 Acc: 0.9268\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.4093 Acc: 0.8633\n",
            "val Loss: 0.2571 Acc: 0.9268\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.4733 Acc: 0.8550\n",
            "val Loss: 0.2421 Acc: 0.9317\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.3690 Acc: 0.8867\n",
            "val Loss: 0.2292 Acc: 0.9366\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.3919 Acc: 0.8633\n",
            "val Loss: 0.2271 Acc: 0.9317\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.3265 Acc: 0.8883\n",
            "val Loss: 0.2265 Acc: 0.9317\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.3326 Acc: 0.8850\n",
            "val Loss: 0.2244 Acc: 0.9366\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.3440 Acc: 0.8917\n",
            "val Loss: 0.2231 Acc: 0.9366\n",
            "\n",
            "Training complete in 5m 16s\n",
            "Best val Acc: 0.936585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the best value accuracy with increased Epochs os 0.93. This is indeed good. "
      ],
      "metadata": {
        "id": "fakMDWb6EYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy_matrix(model_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0RsGv7fEkLy",
        "outputId": "0a6b4906-96c3-4bab-f74e-d3713961f839"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Class  Precision    Recall\n",
            "0        accordion   1.000000  0.937500\n",
            "1             bass   0.928571  0.866667\n",
            "2           camera   1.000000  0.909091\n",
            "3        crocodile   0.500000  1.000000\n",
            "4   crocodile_head   0.909091  0.666667\n",
            "5              cup   0.941176  1.000000\n",
            "6      dollar_bill   1.000000  1.000000\n",
            "7              emu   1.000000  0.928571\n",
            "8       gramophone   1.000000  0.916667\n",
            "9         hedgehog   0.928571  0.928571\n",
            "10        nautilus   1.000000  0.937500\n",
            "11           pizza   1.000000  1.000000\n",
            "12         pyramid   0.882353  1.000000\n",
            "13       sea_horse   0.882353  1.000000\n",
            "14   windsor_chair   1.000000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy matrix for precision and recall has significantly increased for per class accuracies."
      ],
      "metadata": {
        "id": "BhzwLNTbEoWM"
      }
    }
  ]
}